{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging \n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from peft import LoraConfig, LoraModel, AdaLoraModel, AdaLoraConfig, IA3Model, IA3Config\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_dataset(file_path:str):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as fp:\n",
    "        id, questions, answers, text, input_id = json.load(fp)\n",
    "        \n",
    "        data_dict['id'] = id\n",
    "        data_dict['questions'] = questions\n",
    "        data_dict['answers'] = answers\n",
    "        data_dict['text'] = text\n",
    "        data_dict['input_ids'] = input_id\n",
    "        \n",
    "    \n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def load_model(base_model):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    return base_model, tokenizer\n",
    "\n",
    "def prepare_lora_model(base_model, lora_config:LoraConfig, name:str) -> LoraModel:\n",
    "    return LoraModel(base_model, lora_config, name)\n",
    "\n",
    "def prepare_adalora_model(base_model:AutoModelForCausalLM, adalora_config:AdaLoraConfig, name:str) -> AdaLoraModel:\n",
    "    return AdaLoraModel(base_model, adalora_config, name)\n",
    "\n",
    "def prepare_ia3_model(base_model: AutoModelForCausalLM, ia3_config: IA3Config, name:str) -> IA3Model:\n",
    "    return IA3Model(ia3_config, base_model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_model, gemma_tokenizer = load_model(\"google/gemma-7b\")\n",
    "llama2, llama2_tokenizer = load_model(\"meta-llama/Llama-2-7b-hf\")\n",
    "mistral_model, mistral_tokenizer = load_model(\"mistralai/Mistral-7B-v0.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EfficientLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
