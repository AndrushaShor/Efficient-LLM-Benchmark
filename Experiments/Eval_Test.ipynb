{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "import bitsandbytes\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from trl import SFTTrainer\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cosine_similarity = evaluate.load(\"bertscore\")\n",
    "\n",
    "from Experiments.run_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_processed_dataset(file_path:str) -> Dataset:\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as fp:\n",
    "        id, questions, answers = json.load(fp)\n",
    "\n",
    "        data_dict['id'] = id\n",
    "        data_dict['questions'] = questions\n",
    "        data_dict['answers'] = answers\n",
    "\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def load_tokenized_dataset(file_path:str) -> Dataset:\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as fp:\n",
    "        id, questions, answers, text, input_id = json.load(fp)\n",
    "\n",
    "        data_dict['id'] = id\n",
    "        data_dict['questions'] = questions\n",
    "        data_dict['answers'] = answers\n",
    "        data_dict['text'] = text\n",
    "        data_dict['input_ids'] = input_id\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def load_datasets_from_directory(directory_path: str, type='tokenized') -> tuple:\n",
    "    \n",
    "    expected_files = {\"train.json\", \"dev.json\", \"test.json\"}\n",
    "    actual_files = set(os.listdir(directory_path))\n",
    "    \n",
    "    if expected_files != actual_files:\n",
    "        raise ValueError(f\"Directory must contain exactly these files: {expected_files}\")\n",
    "    \n",
    "    if type == 'tokenized':\n",
    "        train_dataset = load_tokenized_dataset(os.path.join(directory_path, \"train.json\"))\n",
    "        dev_dataset = load_tokenized_dataset(os.path.join(directory_path, \"dev.json\"))\n",
    "        test_dataset = load_tokenized_dataset(os.path.join(directory_path, \"test.json\"))\n",
    "    else:\n",
    "        train_dataset = load_processed_dataset(os.path.join(directory_path, \"train.json\"))\n",
    "        dev_dataset = load_processed_dataset(os.path.join(directory_path, \"dev.json\"))\n",
    "        test_dataset = load_processed_dataset(os.path.join(directory_path, \"test.json\"))\n",
    "\n",
    "    return (train_dataset, dev_dataset, test_dataset)\n",
    "\n",
    "def load_model(base_model: str, bnb_config:BitsAndBytesConfig=None, on_gpu:bool=False, use_cache:bool=False, pretraining_tp:int=1) -> AutoModelForCausalLM:\n",
    "    if on_gpu:\n",
    "        print(\"in here\")\n",
    "        base_model_loaded = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "        print(base_model)\n",
    "    else:\n",
    "        base_model_loaded = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "    base_model_loaded.config.use_cache = use_cache\n",
    "    base_model_loaded.config.pretraining_tp = pretraining_tp\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return base_model_loaded, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(scores:list):\n",
    "    num_correct = []\n",
    "    for score in scores:\n",
    "        if score == 1:\n",
    "            num_correct.append(1)\n",
    "        else:\n",
    "            num_correct.append(0)\n",
    "    \n",
    "    accuracy = 100.0 * (sum(num_correct) / len(num_correct))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def strip_output_text(output:str, model_name:str):\n",
    "    model_to_insert_point = {\n",
    "        'google/gemma-7b': [\"Answer:\", \"Explanation:\"],\n",
    "        'meta-llama/Llama-2-7b-hf': \"<s>\",\n",
    "        'mistralai/Mistral-7B-v0.1': \"[INST]\"\n",
    "    }\n",
    "    # Returns the whole input string as well; cut off this part\n",
    "    out = output[output.find(model_to_insert_point[model_name][0]):output.find(model_to_insert_point[model_name][1])]\n",
    "    for repl in ['The answer is:', 'Explanation', '\\n']:\n",
    "        out = out.replace(repl, '')\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict(trained_model:SFTTrainer, tokenizer:AutoTokenizer, eval_sample:Dataset, model_name:str):\n",
    "    reqd_cols = ['id', 'questions', 'answers', 'text', 'input_ids', 'prompt_tokenizations', 'original_dataset']\n",
    "    assert list(eval_sample.features.keys()) == reqd_cols, f\"Eval Data needs the following columsn: {reqd_cols}, but instead has { list(eval_sample.features.keys()) }\"\n",
    "\n",
    "    predictions = []\n",
    "    for inp in eval_sample['prompt_tokenizations']:\n",
    "        inp = torch.tensor(inp, dtype=int)\n",
    "        outp = trained_model.generate(inp, max_new_tokens=20, return_dict_in_generate=True, output_scores=True)\n",
    "        pred = tokenizer.batch_decode(outp['sequences'], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(strip_output_text(pred[0], model_name))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_predictions(eval_sample:Dataset, predictions):\n",
    "    \n",
    "    pred_ds = Dataset.from_dict({'predictions': predictions})\n",
    "\n",
    "    pred_ds = datasets.concatenate_datasets([eval_sample, pred_ds], axis=1)\n",
    "\n",
    "    original_datasets = set(pred_ds['original_datasets'])\n",
    "    filt = {}\n",
    "    for ds in original_datasets:\n",
    "        filt[ds] = pred_ds(lambda ex: ex['original_datasets'] == ds)\n",
    "    \n",
    "    scores = []\n",
    "    for ds, data in filt.items():\n",
    "        scores.append(compute_metrics(ds, pred_ds['predictions'], pred_ds['answers']))\n",
    "    \n",
    "    accuracy = compute_accuracy(scores)\n",
    "    return scores, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_rouge(predictions:list, ground_truth:list):\n",
    "    scores = rouge.compute(predictions=predictions, references=ground_truth, use_aggregator=False)\n",
    "    return scores['rougeL'] # longest common subsequence-based ROUGE\n",
    "    \n",
    "\n",
    "def compute_similarity(predictions:list, ground_truth:list):\n",
    "    scores = cosine_similarity.compute(predictions=predictions, references=ground_truth, model_type=\"distilbert-base-uncased\")\n",
    "    return scores['f1']\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(original_dataset:str, predictions:list, ground_truth:list):\n",
    "\n",
    "    ds_metric_map = {\n",
    "        'ai2_science_elementary': 'cosine_similarity',\n",
    "        'ai2_science_middle': 'cosine_similarity',\n",
    "        'arc_easy': 'cosine_similarity',\n",
    "        'arc_hard': 'cosine_similarity',\n",
    "        'narrativeqa': 'rouge',\n",
    "        'openbookqa' : 'cosine_similarity',\n",
    "        'race_string': 'cosine_similarity'}\n",
    "    \n",
    "    assert original_dataset in ds_metric_map, f\"Please define a metric mapping for dataset {original_dataset}\"\n",
    "    \n",
    "    metric = ds_metric_map[original_dataset]\n",
    "    \n",
    "    if metric == 'rouge':\n",
    "        scores = compute_rouge(predictions, ground_truth)\n",
    "    elif metric == 'cosine_similarity':\n",
    "        scores = compute_similarity(predictions, ground_truth)\n",
    "    \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_prompt_icl(hf_model: str, ds: Dataset, experiment, k_shot: int=1, \n",
    "               max_k_shot_token_length=200, seed=42, sample: int=1000):\n",
    "    ds = ds.shuffle(seed=seed)\n",
    "    eval_sample = ds.select(range(sample))\n",
    "\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(hf_model, device_map={\"\": 0})\n",
    "    \n",
    "    def filter_by_token_length(example):\n",
    "        tokens = loaded_tokenizer(example['text'], return_tensors=\"pt\", truncation=False)\n",
    "        return tokens.input_ids.size(1) <= max_k_shot_token_length\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'Running prompt injection for: {experiment}')\n",
    "    prompt_insert = \"Answer this question truthfully:\"\n",
    "    \n",
    "    if experiment == 'zero_shot':\n",
    "        prompt_insert = \"Answer the question truthfully:\"\n",
    "        results = process_samples(eval_sample, hf_model, prompt_insert, loaded_tokenizer)\n",
    "\n",
    "    elif experiment == 'k_shot':\n",
    "        filtered_dataset_for_k_shot =  ds.filter(filter_by_token_length) \n",
    "        print(f\"Number of examples in the dataset: {len(filtered_dataset_for_k_shot)}\")\n",
    "        if len(filtered_dataset_for_k_shot) < k_shot:\n",
    "            raise ValueError(f\"Dataset has less than {k_shot} examples\")\n",
    "        \n",
    "        prompt_insert = \"Answer the question truthfully. Follow these examples:\"\n",
    "        prompt_insert += \"\\n\".join(filtered_dataset_for_k_shot['questions'][:k_shot])\n",
    "        prompt_insert += \"\\n\"\n",
    "        prompt_insert += 'Question:'\n",
    "        \n",
    "        results = process_samples(eval_sample, hf_model, prompt_insert, loaded_tokenizer)\n",
    "    print(results['prompt_tokenizations'])\n",
    "    eval_sample = datasets.concatenate_datasets([eval_sample, results], axis=1)\n",
    "\n",
    "    return eval_sample\n",
    "\n",
    "def process_samples(sample_data, model_name, prompt_insert, tokenizer):\n",
    "    model_to_insert_point = {\n",
    "        'google/gemma-7b': \"user\",\n",
    "        'meta-llama/Llama-2-7b-hf': \"<s>\",\n",
    "        'mistralai/Mistral-7B-v0.1': \"[INST]\"\n",
    "    }\n",
    "    \n",
    "    original_dataset = []\n",
    "    new_tokenizations = []\n",
    "\n",
    "    for example in sample_data:\n",
    "        text = example['questions']\n",
    "        insertion_point = text.find(model_to_insert_point[model_name]) + len(model_to_insert_point[model_name])\n",
    "        new_text = text[:insertion_point] + \" \" + prompt_insert + \" \" + text[insertion_point:]\n",
    "        \n",
    "        inputs = tokenizer(new_text, return_tensors=\"pt\")  \n",
    "        original_dataset.append(example['id'].split('-')[0])\n",
    "        new_tokenizations.append(inputs.input_ids)\n",
    "    processed_samples = {'prompt_tokenizations': new_tokenizations, 'original_dataset': original_dataset}\n",
    "    out = Dataset.from_dict(processed_samples)\n",
    "    print(out['prompt_tokenizations'])\n",
    "    return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized test datasets\n",
    "gemma_test = load_tokenized_dataset(os.path.join(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Gemma\", \"test.json\"))\n",
    "llama_test = load_tokenized_dataset(os.path.join(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Llama\", \"test.json\"))\n",
    "mistral_test = load_tokenized_dataset(os.path.join(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Mistral\", \"test.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prompt for zero-shot (regular inference prompt)\n",
    "gemma_test_processed = preprocess_prompt_icl(\"google/gemma-7b\", ds=gemma_test, experiment='zero_shot', sample=20)\n",
    "gemma_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31317\n",
      "--------------------------------------------------------------\n",
      "narrativeqa-test-0\n",
      "--------------------------------------------------------------\n",
      "<bos><start_of_turn>user\n",
      "who is mark hunter? \\n  mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \"everybody knows\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \"hard harry\" or \"happy harry hard-on,\" as mark refers to himself, until nora diniro (mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicideâat the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \"problem students,\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \"talk hard.\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country.<end_of_turn>\n",
      "\n",
      "\n",
      "--------------------------------------------------------------\n",
      "<start_of_turn>model\n",
      "he is a high school student in phoenix.<end_of_turn>\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(gemma_test_processed['id'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(gemma_test_processed['questions'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(gemma_test_processed['answers'][0])\n",
    "print('----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small model (just for testing)\n",
    "gemma_model, gemma_tokenizer = load_model(base_model=\"google/gemma-2b\", bnb_config=CONFIG_4BITS_NORM, on_gpu=False, use_cache=False, pretraining_tp=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EfficientLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
