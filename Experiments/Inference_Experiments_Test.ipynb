{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to test speculative decoding functions work with a finetuned gemma-2b for faster inference on text\n",
    "# Procedure in general for Speculative Decoding:\n",
    "    #1. Finetune Gemma-2b on UnifiedQA Dataset \n",
    "    #2. Use Gemma-2b as the draft model for Gemma-7b to see number of tokens w/ latency\n",
    "    #3. To measure throughput: https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging \n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from peft import LoraConfig, LoraModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from trl import SFTTrainer\n",
    "from quantization import CONFIG_4BITS, CONFIG_4BITS_NESTED, CONFIG_4BITS_NORM, CONFIG_8BITS, CONFIG_4BITS_NORM_NESTED\n",
    "from run_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_train_dataset = load_tokenized_dataset(\"/home/andrusha/Desktop/DL Research/Efficient-LLM-Benchmark/UnifiedQA Data Curation/tokenized/Gemma/train.json\")\n",
    "gemma_dev_dataset = load_tokenized_dataset(\"/home/andrusha/Desktop/DL Research/Efficient-LLM-Benchmark/UnifiedQA Data Curation/tokenized/Gemma/dev.json\")\n",
    "gemma_test_dataset = load_tokenized_dataset(\"/home/andrusha/Desktop/DL Research/Efficient-LLM-Benchmark/UnifiedQA Data Curation/tokenized/Gemma/test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_model, gemma_tokenizer = load_model(base_model=\"google/gemma-7b\", bnb_config=CONFIG_4BITS_NORM_NESTED, on_gpu=True, use_cache=False, pretraining_tp=1) \n",
    "\n",
    "gemma_model_2b, gemma_tokenizer = load_model(base_model=\"google/gemma-2b\", bnb_config=CONFIG_4BITS_NORM_NESTED, on_gpu=False, use_cache=False, pretraining_tp=1) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2\n",
    "inputs = gemma_tokenizer(\"Generate a python code that accepts a list of numbers and returns the sum.\", return_tensors='pt', return_attention_mask=False)\n",
    "speculative_decoding(gemma_model, gemma_model_2b, inputs, gemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput(gemma_model, gemma_model_2b, gemma_tokenizer, inputs, 200, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_model_of_gpu(gemma_model)\n",
    "del_model_of_gpu(gemma_model_2b)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
