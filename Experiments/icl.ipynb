{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A notebook to evaluate In-Context Learning Question Answering capabilities of base models on UnifiedQA\n",
    "'''\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# from Experiments.quantization import CONFIG_4BITS, CONFIG_4BITS_NESTED, CONFIG_4BITS_NORM, CONFIG_8BITS, CONFIG_4BITS_NORM_NESTED\n",
    "# from Experiments.run_utils import *\n",
    "# from Experiments.eval_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def load_datasets_from_directory(directory_path: str, type='tokenized') -> tuple:\n",
    "    \n",
    "    expected_files = {\"train.json\", \"dev.json\", \"test.json\"}\n",
    "    actual_files = set(os.listdir(directory_path))\n",
    "    \n",
    "    if expected_files != actual_files:\n",
    "        raise ValueError(f\"Directory must contain exactly these files: {expected_files}\")\n",
    "    \n",
    "    if type == 'tokenized':\n",
    "        train_dataset = load_tokenized_dataset(os.path.join(directory_path, \"train.json\"))\n",
    "        dev_dataset = load_tokenized_dataset(os.path.join(directory_path, \"dev.json\"))\n",
    "        test_dataset = load_tokenized_dataset(os.path.join(directory_path, \"test.json\"))\n",
    "    else:\n",
    "        train_dataset = load_processed_dataset(os.path.join(directory_path, \"train.json\"))\n",
    "        dev_dataset = load_processed_dataset(os.path.join(directory_path, \"dev.json\"))\n",
    "        test_dataset = load_processed_dataset(os.path.join(directory_path, \"test.json\"))\n",
    "\n",
    "    return (train_dataset, dev_dataset, test_dataset)\n",
    "\n",
    "def load_tokenized_dataset(file_path:str) -> Dataset:\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as fp:\n",
    "        id, questions, answers, text, input_id = json.load(fp)\n",
    "\n",
    "        data_dict['id'] = id\n",
    "        data_dict['questions'] = questions\n",
    "        data_dict['answers'] = answers\n",
    "        data_dict['text'] = text\n",
    "        data_dict['input_ids'] = input_id\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "def load_model(base_model: str, bnb_config:BitsAndBytesConfig=None, on_gpu:bool=False, use_cache:bool=False, pretraining_tp:int=1) -> AutoModelForCausalLM:\n",
    "    if on_gpu:\n",
    "        print(\"in here\")\n",
    "        base_model_loaded = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "        print(base_model)\n",
    "    else:\n",
    "        base_model_loaded = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "    base_model_loaded.config.use_cache = use_cache\n",
    "    base_model_loaded.config.pretraining_tp = pretraining_tp\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return base_model_loaded, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/andrusha/Desktop/DL Research/Efficient-LLM-Benchmark/Experiments/UnifiedQA Data Curation/tokenized/Gemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset for testing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _, _, gemma_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_datasets_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/UnifiedQA Data Curation/tokenized/Gemma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m _, _, llama_test\u001b[38;5;241m=\u001b[39m load_datasets_from_directory(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/UnifiedQA Data Curation/tokenized/Llama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m _, _, mistral_test \u001b[38;5;241m=\u001b[39m load_datasets_from_directory(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/UnifiedQA Data Curation/tokenized/Mistral\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_datasets_from_directory\u001b[0;34m(directory_path, type)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_datasets_from_directory\u001b[39m(directory_path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m      5\u001b[0m     expected_files \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 6\u001b[0m     actual_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expected_files \u001b[38;5;241m!=\u001b[39m actual_files:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory must contain exactly these files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/andrusha/Desktop/DL Research/Efficient-LLM-Benchmark/Experiments/UnifiedQA Data Curation/tokenized/Gemma'"
     ]
    }
   ],
   "source": [
    "# Load dataset for testing\n",
    "_, _, gemma_test = load_datasets_from_directory(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Gemma\", type='tokenized')\n",
    "_, _, llama_test= load_datasets_from_directory(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Llama\", type='tokenized')\n",
    "_, _, mistral_test = load_datasets_from_directory(f\"{os.getcwd()}/UnifiedQA Data Curation/tokenized/Mistral\", type='tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31317\n",
      "31317\n",
      "31317\n"
     ]
    }
   ],
   "source": [
    "print(len(gemma_test))\n",
    "print(len(llama_test))\n",
    "print(len(mistral_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narrativeqa-test-0\n",
      "--------------------------------------------------------------\n",
      "<bos><start_of_turn>user\n",
      "who is mark hunter? \\n  mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \"everybody knows\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \"hard harry\" or \"happy harry hard-on,\" as mark refers to himself, until nora diniro (mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicideâat the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \"problem students,\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \"talk hard.\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country.<end_of_turn>\n",
      "\n",
      "\n",
      "--------------------------------------------------------------\n",
      "<start_of_turn>model\n",
      "he is a high school student in phoenix.<end_of_turn>\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(gemma_test['id'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(gemma_test['questions'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(gemma_test['answers'][0])\n",
    "print('----------------------------------------------------------------------------')\n",
    "print(gemma_test)\n",
    "# prompt_insert = \"Answer this question truthfully:\"\n",
    "# text = gemma_test['questions'][0]\n",
    "# insertion_point = text.find(\"user\") + len(\"user\")\n",
    "# new_text = text[:insertion_point] + \" \" + prompt_insert + text[insertion_point:]\n",
    "# new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narrativeqa-test-0\n",
      "--------------------------------------------------------------\n",
      "<s>Input:\n",
      "who is mark hunter? \\n  mark hunter (slater), a high school student in a sleepy suburb of phoenix, arizona, starts an fm pirate radio station that broadcasts from the basement of his parents' house. mark is a loner, an outsider, whose only outlet for his teenage angst and aggression is his unauthorized radio station. his pirate station's theme song is \"everybody knows\" by leonard cohen and there are glimpses of cassettes by such alternative musicians as the jesus and mary chain, camper van beethoven, primal scream, soundgarden, ice-t, bad brains, concrete blonde, henry rollins, and the pixies. by day, mark is seen as a loner, hardly talking to anyone around him; by night, he expresses his outsider views about what is wrong with american society. when he speaks his mind about what is going on at his school and in the community, more and more of his fellow students tune in to hear his show.nobody knows the true identity of \"hard harry\" or \"happy harry hard-on,\" as mark refers to himself, until nora diniro (mathis), a fellow student, tracks him down and confronts him the day after a student named malcolm commits suicide after harry attempts to reason with him. the radio show becomes increasingly popular and influential after harry confronts the suicide head-on, exhorting his listeners to do something about their problems instead of surrendering to them through suicideâat the crescendo of his yelled speech, an overachieving student named paige woodward (who has been a constant listener) jams her various medals and accolades into a microwave and turns it on. she then sits, watching the awards cook until the microwave explodes, injuring her. while this is happening, other students act out in cathartic release.eventually, the radio show causes so much trouble in the community that the fcc is called in to investigate. during the fracas, it is revealed that the school's principal (annie ross) has been expelling \"problem students,\" namely, students with below-average standardized test scores, in an effort to boost the district's test scores while still keeping their names on the rolls (a criminal offense) in order to retain government funding.realizing he has started something huge, mark decides it is up to him to end it. he dismantles his radio station and attaches it to his mother's old jeep, creating a mobile transmitter so his position can't be triangulated. pursued by the police and the fcc, nora drives the jeep around while mark broadcasts. the harmonizer he uses to disguise his voice breaks, and with no time left to fix it, mark decides to broadcast his final message as himself. they finally drive up to the crowd of protesting students, and mark tells them that the world belongs to them and that they should make their own future. the police step in and arrest mark and nora. as they are taken away, mark reminds the students to \"talk hard.\" as the film ends, the voices of other students (and even one of the teachers) speak as intros for their own independent stations, which can be heard broadcasting across the country.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Output:\n",
      "he is a high school student in phoenix.\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(llama_test['id'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(llama_test['questions'][0])\n",
    "print('--------------------------------------------------------------')\n",
    "print(llama_test['answers'][0])\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "# prompt_insert = \"Answer this question truthfully:\"\n",
    "# text = llama_test['questions'][0]\n",
    "# insertion_point = text.find(\"<s>\") + len(\"<s>\")\n",
    "# new_text = text[:insertion_point] + \" \" + prompt_insert + \" \" +text[insertion_point:]\n",
    "# new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa-test-183\n",
      "--------------------------------------------------------------\n",
      "<s>[INST] what type of useful product can be made from the moving winds? \\n (a) wood (b) bananas (c) electricity (d) metal [/INST]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------\n",
      "electricity\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(mistral_test['id'][31000])\n",
    "print('--------------------------------------------------------------')\n",
    "print(mistral_test['questions'][31000])\n",
    "print('--------------------------------------------------------------')\n",
    "print(mistral_test['answers'][31000])\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "# prompt_insert = \"Answer this question truthfully:\"\n",
    "# text = mistral_test['questions'][0]\n",
    "# insertion_point = text.find(\"[INST]\") + len(\"[INST]\")\n",
    "# new_text = text[:insertion_point] + \" \" + prompt_insert + \" \" +text[insertion_point:]\n",
    "# new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Quantized models \n",
    "# Testing all Quantization varients for hf model\n",
    "# NOTE: For unquantized, you need an A100 GPU - 7b models take up roughly 26.8 GB of memory.\n",
    "\n",
    "# https://towardsdatascience.com/in-context-learning-approaches-in-large-language-models-9c0c53b116a1\n",
    "# https://rahulrajpvr7d.medium.com/zero-shot-one-shot-and-few-shot-learning-with-examples-8a3efdcbb158\n",
    "# https://huggingface.co/docs/transformers/en/tasks/language_modeling\n",
    "\n",
    "# Gemma context length = 8192\n",
    "# Llama 2 context length = 4096\n",
    "# mistral 7b context length = 8192 \n",
    "\n",
    "def preprocess_prompt_icl(hf_model: str, ds: Dataset, experiment, k_shot: int=1, \n",
    "               max_k_shot_token_length=200, seed=42, sample: int=1000):\n",
    "    ds = ds.shuffle(seed=seed)\n",
    "    eval_sample = ds.select(range(sample))\n",
    "\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(hf_model, device_map={\"\": 0})\n",
    "    \n",
    "    def filter_by_token_length(example):\n",
    "        tokens = loaded_tokenizer(example['text'], return_tensors=\"pt\", truncation=False)\n",
    "        return tokens.input_ids.size(1) <= max_k_shot_token_length\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'Running prompt injection for: {experiment}')\n",
    "    prompt_insert = \"Answer this question truthfully:\"\n",
    "    \n",
    "    if experiment == 'zero_shot':\n",
    "        prompt_insert = \"Answer the question truthfully:\"\n",
    "        results = process_samples(eval_sample, hf_model, prompt_insert, loaded_tokenizer)\n",
    "\n",
    "    elif experiment == 'k_shot':\n",
    "        filtered_dataset_for_k_shot =  ds.filter(filter_by_token_length) \n",
    "        print(f\"Number of examples in the dataset: {len(filtered_dataset_for_k_shot)}\")\n",
    "        if len(filtered_dataset_for_k_shot) < k_shot:\n",
    "            raise ValueError(f\"Dataset has less than {k_shot} examples\")\n",
    "        \n",
    "        prompt_insert = \"Answer the question truthfully. Follow these examples:\"\n",
    "        prompt_insert += \"\\n\".join(filtered_dataset_for_k_shot['questions'][:k_shot])\n",
    "        prompt_insert += \"\\n\"\n",
    "        prompt_insert += 'Question:'\n",
    "        \n",
    "        results = process_samples(eval_sample, hf_model, prompt_insert, loaded_tokenizer)\n",
    "    print(results['prompt_tokenizations'])\n",
    "    eval_sample = datasets.concatenate_datasets([eval_sample, results], axis=1)\n",
    "\n",
    "    return eval_sample\n",
    "\n",
    "def process_samples(sample_data, model_name, prompt_insert, tokenizer):\n",
    "    model_to_insert_point = {\n",
    "        'google/gemma-7b': \"user\",\n",
    "        'meta-llama/Llama-2-7b-hf': \"<s>\",\n",
    "        'mistralai/Mistral-7B-v0.1': \"[INST]\"\n",
    "    }\n",
    "    \n",
    "    original_dataset = []\n",
    "    new_tokenizations = []\n",
    "\n",
    "    for example in sample_data:\n",
    "        text = example['questions']\n",
    "        insertion_point = text.find(model_to_insert_point[model_name]) + len(model_to_insert_point[model_name])\n",
    "        new_text = text[:insertion_point] + \" \" + prompt_insert + \" \" + text[insertion_point:]\n",
    "        \n",
    "        inputs = tokenizer(new_text, return_tensors=\"pt\")  \n",
    "        original_dataset.append(example['id'].split('-')[0])\n",
    "        new_tokenizations.append(inputs.input_ids)\n",
    "    processed_samples = {'prompt_tokenizations': new_tokenizations, 'original_dataset': original_dataset}\n",
    "    out = Dataset.from_dict(processed_samples)\n",
    "    print(out['prompt_tokenizations'])\n",
    "    return out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
